{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "interfer1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "EY1DWQaIwpGx",
        "outputId": "197a6aaa-ba04-4192-c6a1-a58f09e8451d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DGr9tRmM-Jf6",
        "outputId": "03c5cdf9-0bdf-4837-902f-4d53426264f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gym==0.12.1\n",
        "!pip install -e git+https://dmsoroki@bitbucket.org/dmsoroki/gym_interf.git@master#egg=gym_interf\n",
        "!pip install opencv-python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym==0.12.1 in /usr/local/lib/python3.6/dist-packages (0.12.1)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.12.1) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.12.1) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.12.1) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.12.1) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.12.1) (2.21.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym==0.12.1) (0.16.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.12.1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.12.1) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.12.1) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.12.1) (1.24.2)\n",
            "Obtaining gym_interf from git+https://dmsoroki@bitbucket.org/dmsoroki/gym_interf.git@master#egg=gym_interf\n",
            "  Updating ./src/gym-interf clone (to revision master)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym_interf) (0.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gym_interf) (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from gym_interf) (3.0.3)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (from gym_interf) (1.9.6)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym_interf) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym_interf) (1.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym_interf) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym_interf) (2.21.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gym_interf) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gym_interf) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gym_interf) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gym_interf) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym->gym_interf) (0.16.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym_interf) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym_interf) (1.24.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym_interf) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym_interf) (2019.3.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->gym_interf) (40.9.0)\n",
            "Installing collected packages: gym-interf\n",
            "  Found existing installation: gym-interf 0.1\n",
            "    Can't uninstall 'gym-interf'. No files were found to uninstall.\n",
            "  Running setup.py develop for gym-interf\n",
            "Successfully installed gym-interf\n",
            "/bin/bash: pin: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PzozJqGXya0_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_interf\n",
        "import cv2\n",
        "\n",
        "env = gym.make('interf-v1')\n",
        "\n",
        "class Wrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        type(env).n_points = 256\n",
        "        type(env).x_min = -4\n",
        "        type(env).x_max = 4\n",
        "        super().__init__(env)\n",
        "    \n",
        "    \n",
        "    def observation(self, state):\n",
        "        result = np.ndarray(shape=(16,64,64), dtype=np.uint8)\n",
        "        for i, image in enumerate(state):\n",
        "            resized = cv2.resize(image, (64, 64))\n",
        "            result[i] = resized\n",
        "        return result\n",
        "    \n",
        "env = Wrapper(env)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TReuIobwB1y2",
        "outputId": "a6213a57-0bf3-4b2e-fd05-0e5a910a3139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import random\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from collections import deque\n",
        "from itertools import count\n",
        "from copy import deepcopy\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display    \n",
        "print(\"Is python : {}\".format(is_ipython))\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device : {}\".format(device))\n",
        "\n",
        "\n",
        "ACTIONS_NUM = 8\n",
        "print(\"Number of actions : {}\".format(ACTIONS_NUM))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is python : True\n",
            "Device : cuda\n",
            "Number of actions : 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "B855txz_CF9O",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory:\n",
        "\n",
        "    def __init__(self, capacity = 40000):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dTf-oaazCM4G",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, in_channels=16, num_actions=ACTIONS_NUM):\n",
        "        \n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.conv4 = nn.Conv2d(64,1024,kernel_size=4,stride=1)\n",
        "        self.advantage = nn.Linear(512, num_actions)\n",
        "        self.value = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float() / 255\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        advantage,value = torch.split(x,512,dim=1)\n",
        "        \n",
        "        advantage = advantage.view(advantage.shape[0],-1)\n",
        "        value = value.view(value.shape[0],-1)\n",
        "        \n",
        "        advantage = self.advantage(advantage)\n",
        "        value = self.value(value)\n",
        "        q_value = value.expand(value.shape[0],ACTIONS_NUM) +\\\n",
        "        advantage-torch.mean(advantage,dim=1).unsqueeze(1).expand(advantage.shape[0],ACTIONS_NUM)\n",
        "        return q_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qnjzA1aJCzd3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "policy_net = DQN().to(device)\n",
        "\n",
        "state = torch.load('drive/My Drive/colab/policy_net2')\n",
        "policy_net.load_state_dict(state)\n",
        "policy_net.eval()\n",
        "\n",
        "target_net = DQN().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer =optim.Adam(policy_net.parameters(),lr=1e-5)\n",
        "\n",
        "memory = ReplayMemory()\n",
        "\n",
        "def select_action(state, eps_threshold):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            state=state.float()\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(ACTIONS_NUM)]], device=device, dtype=torch.long)\n",
        "\n",
        "train_rewards = []\n",
        "\n",
        "mean_size = 100\n",
        "mean_step = 1\n",
        "\n",
        "def plot_rewards(rewards = train_rewards, name = \"Train\"):\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    plt.title(name)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(rewards)\n",
        "    if len(rewards) > mean_size:\n",
        "        means = np.array([rewards[i:i+mean_size:] for i in range(0, len(rewards) - mean_size, mean_step)]).mean(1)\n",
        "        means = np.concatenate((np.zeros(mean_size - 1), means))\n",
        "        plt.plot(means)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "q8wr_6TvC4JP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.99\n",
        "\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    \n",
        "    \n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "    \n",
        "    \n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    \n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    \n",
        "    state_batch=state_batch.float()\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "    \n",
        "    \n",
        "    non_final_next_states=non_final_next_states.float()\n",
        "    next_state_values = torch.zeros((BATCH_SIZE,1), device=device)\n",
        "    next_state_actions = torch.zeros(BATCH_SIZE,dtype=torch.long, device=device)\n",
        "    \n",
        "    next_state_actions[non_final_mask] = policy_net(non_final_next_states).max(1)[1]\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, next_state_actions[non_final_mask].unsqueeze(1))\n",
        "    next_state_values=next_state_values.squeeze(1)\n",
        "    \n",
        "    \n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "    \n",
        "    \n",
        "    \n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1).detach())\n",
        "    \n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "    \n",
        "    del non_final_mask\n",
        "    del non_final_next_states\n",
        "    del state_batch\n",
        "    del action_batch\n",
        "    del reward_batch\n",
        "    del state_action_values\n",
        "    del next_state_values\n",
        "    del expected_state_action_values\n",
        "    del loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GK6tpBmXHeQd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  TEST_EPS = 0.005\n",
        "  state = env.reset() \n",
        "  total_reward = 0\n",
        "  for i in count():\n",
        "    state = np.array(state,dtype=np.float32)\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "    state = state.unsqueeze(0)\n",
        "    action = select_action(state, TEST_EPS)\n",
        "    state, _, done, info = env.step(action)\n",
        "    reward = -1.0+info['visib']\n",
        "    total_reward+=reward\n",
        "    if done:\n",
        "       break\n",
        "  return i+1,total_reward,info['visib']\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Bs3QbaD-C8vH",
        "outputId": "a0ad451e-0cb2-42a2-cae4-7e69bc2ddd41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "NUM_EPISODES = 200000\n",
        "\n",
        "\n",
        "OPTIMIZE_MODEL_STEP = 4\n",
        "\n",
        "\n",
        "TARGET_UPDATE=10000\n",
        "\n",
        "\n",
        "\n",
        "STEPS_BEFORE_TRAIN = 30000\n",
        "\n",
        "\n",
        "\n",
        "EPS_START = 1\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = 1000000\n",
        "\n",
        "EPS_START_v2 = 0.1\n",
        "EPS_END_v2 = 0.01\n",
        "\n",
        "policy_net.train()\n",
        "target_net.eval()\n",
        "test_rewards = []\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "for e in range(NUM_EPISODES):\n",
        "\n",
        "    state = env.reset() \n",
        "    state = np.array(state,dtype=np.float32)\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "    state = state.unsqueeze(0)\n",
        "    ep_rewards=0\n",
        "    \n",
        "    for t in range(180000):\n",
        "\n",
        "        \n",
        "        if steps_done<EPS_DECAY:\n",
        "            if steps_done>STEPS_BEFORE_TRAIN:\n",
        "                fraction=min(float(steps_done)/EPS_DECAY,1)\n",
        "                eps_threshold= EPS_START + (EPS_END - EPS_START) * fraction\n",
        "                action = select_action(state,eps_threshold)\n",
        "            else:\n",
        "                action=torch.tensor([[random.randrange(ACTIONS_NUM)]], device=device, dtype=torch.long)\n",
        "        \n",
        "        else:\n",
        "            fraction=min(float(steps_done)/2*EPS_DECAY,1)\n",
        "            eps_threshold= EPS_START_v2 + (EPS_END_v2 - EPS_START_v2) * fraction\n",
        "            action = select_action(state,eps_threshold)\n",
        "            \n",
        "            \n",
        "        \n",
        "        next_state, _, done,info = env.step(action.item())\n",
        "        reward = -1.0+info['visib']\n",
        "        ep_rewards += reward\n",
        "        \n",
        "        next_state = np.array(next_state,dtype=np.float32)\n",
        "        next_state = torch.tensor(next_state,dtype=torch.float32,device=device)\n",
        "        next_state = next_state.unsqueeze(0)\n",
        "        \n",
        "        \n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        if not done:\n",
        "            memory.push(state, action,next_state, reward)\n",
        "        else:\n",
        "            next_state=None\n",
        "            memory.push(state, action,next_state, reward)  \n",
        "              \n",
        "        steps_done+=1\n",
        "        state=next_state\n",
        "       \n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "        if (steps_done > STEPS_BEFORE_TRAIN) and steps_done % OPTIMIZE_MODEL_STEP == 0:\n",
        "            optimize_model()\n",
        "        \n",
        "\n",
        "        if steps_done % TARGET_UPDATE == 0:\n",
        "            print(\"Target net updated!\")\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "            torch.save(policy_net.state_dict(),'curr_policy_net')\n",
        "        \n",
        "\n",
        "        if done:\n",
        "            train_rewards.append(np.sum(ep_rewards))         \n",
        "            plot_rewards()\n",
        "            break \n",
        "    if e%100==0:\n",
        "      policy_net.eval()\n",
        "      total = 0\n",
        "      val = 0 \n",
        "      visib_end = 0.\n",
        "      for _ in range(10):\n",
        "        res0,res1,final_visib = test()\n",
        "        val+= res0\n",
        "        total += res1\n",
        "        visib_end+=final_visib\n",
        "      policy_net.train()\n",
        "      print('---- steps_done {}  ---- Test_score {} ----- Number of steps needed {} --- final_visib = {}'.format(steps_done,total/10.,val/10.,visib_end/10.))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---- steps_done 200  ---- Test_score -2.102157699473312 ----- Number of steps needed 40.1 --- final_visib = 0.9963893907783223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tShTxj6coes0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(policy_net.state_dict(),'policy_net')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yK8HaznFABO_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k-mLi2jhcrNZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TEST_EPS = 0.005\n",
        " \n",
        "dist_all = []\n",
        "action_all = []\n",
        "visib_all = []\n",
        "steps = []\n",
        "TEST_EPS = 0.0\n",
        "env.reset_actions = 1000\n",
        "env.max_steps = 200\n",
        "for _ in range(100):\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "  for i in count():\n",
        "    state = np.array(state,dtype=np.float32)\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "    state = state.unsqueeze(0)\n",
        "    action = select_action(state, TEST_EPS)\n",
        "    state, _, done, info = env.step(action)\n",
        "    reward = -1.0+info['visib']\n",
        "    total_reward+=reward\n",
        "    # dist_all.append(info['dist'])\n",
        "    # visib_all.append(info['visib'])\n",
        "    # action_all.append(action)\n",
        "    if done:\n",
        "        steps.append(i)\n",
        "        break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "C5eFRpXOqGBS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "steps = np.array(steps)\n",
        "steps[steps<100].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LLcCUMKqrr8d",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.sort(steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FO47zPAvuJOp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "steps = []\n",
        "dist_all = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "J2gUnJMLtjuE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TEST_EPS = 0.005\n",
        " \n",
        "action_all = []\n",
        "visib_all = []\n",
        "TEST_EPS = 0.0\n",
        "env.reset_actions = 5000\n",
        "env.max_steps = 200\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "for i in count():\n",
        "  state = np.array(state,dtype=np.float32)\n",
        "  state = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "  state = state.unsqueeze(0)\n",
        "  action = select_action(state, TEST_EPS)\n",
        "  state, _, done, info = env.step(action)\n",
        "  reward = -1.0+info['visib']\n",
        "  total_reward+=reward\n",
        "  if i == 0:\n",
        "    dist_all.append(info['dist'])\n",
        "  visib_all.append(info['visib'])\n",
        "  action_all.append(action)\n",
        "  if done:\n",
        "    \n",
        "    steps.append(i)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iTO99bq3uEpy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(steps,dist_all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OMqYprvFk_zr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(dist_all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hyjcRNPYlMA5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(visib_all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "91F-GXFWlUBL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visib_all[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dcbFbW9alqjJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}